"""
Real Event Scraper for Antalya
Based on working scrapers from the original rotiva project
Only supports Antalya city
"""

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
from datetime import datetime, timedelta
from urllib.parse import urljoin
import os
from dotenv import load_dotenv
import logging
import re
import time

# .env dosyasÄ±ndan environment variable'larÄ± yÃ¼kle
load_dotenv()

# Logging ayarlarÄ±
import pathlib
log_dir = pathlib.Path('logs')
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler(log_dir / 'scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# MongoDB baÄŸlantÄ±sÄ± (.env'den okunur)
MONGO_URI = os.getenv('MONGO_URI', 'mongodb://localhost:27017/')

# Gemini API Key (.env dosyasÄ±ndan okunur)
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    logger.warning("âš ï¸  GEMINI_API_KEY .env dosyasÄ±nda bulunamadÄ±")

try:
    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
    client.server_info()
    db = client['events_db']
    events_collection = db['events']
    logger.info("âœ… MongoDB connected successfully")
except Exception as e:
    logger.error(f"âŒ MongoDB connection failed: {e}")
    raise

# Gemini AI yapÄ±landÄ±rmasÄ± (opsiyonel - kategorizasyon iÃ§in)
if GEMINI_API_KEY:
    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        # Try gemini-2.5-flash first, fallback to gemini-pro
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
        except:
            try:
                model = genai.GenerativeModel('gemini-pro')
            except:
                model = None
        if model:
            logger.info("âœ… Gemini AI configured")
        else:
            logger.warning("Gemini model could not be initialized")
    except Exception as e:
        logger.warning(f"Gemini AI configuration failed: {e}")
        model = None
else:
    logger.warning("GEMINI_API_KEY bulunamadÄ±, basit kategorizasyon kullanÄ±lacak")
    model = None

# ============ HELPER FUNCTIONS ============

def clean_text(text):
    """Metindeki fazla boÅŸluklarÄ± temizle ve tek satÄ±ra indir"""
    if not text:
        return ''
    return ' '.join(str(text).split())

def parse_date_from_text(text: str):
    """TÃ¼rkÃ§e metinden YYYY-MM-DD formatÄ±nda tarih Ã§Ä±kar. Bulamazsa None dÃ¶ner.
    Ã–rnekler: '23 Ekim', '23 Ekim Per - 20:00', '02 KasÄ±m Paz - 21:00'
    Not: YÄ±l belirtilmemiÅŸse mevcut yÄ±lÄ± varsayar.
    """
    if not text:
        return None
    
    t = text.lower()
    # TÃ¼rkÃ§e ay isimleri ve karÅŸÄ±lÄ±k gelen sayÄ±larÄ±
    month_to_num = {
        'ocak': 1, 'ÅŸubat': 2, 'subat': 2, 'mart': 3, 'nisan': 4, 'mayÄ±s': 5, 'mayis': 5,
        'haziran': 6, 'temmuz': 7, 'aÄŸustos': 8, 'agustos': 8, 'eylÃ¼l': 9, 'eylul': 9,
        'ekim': 10, 'kasÄ±m': 11, 'kasim': 11, 'aralÄ±k': 12, 'aralik': 12,
    }
    # GÃ¼n + Ay formatÄ±nÄ± bul (Ã¶rnek: "23 ekim")
    m = re.search(r"\b(\d{1,2})\s+(ocak|ÅŸubat|subat|mart|nisan|mayÄ±s|mayis|haziran|temmuz|aÄŸustos|agustos|eylÃ¼l|eylul|ekim|kasÄ±m|kasim|aralÄ±k|aralik)\b", t)
    if not m:
        return None
    try:
        day = int(m.group(1))
        month = month_to_num.get(m.group(2))
        if not month:
            return None
        year = datetime.now().year
        dt = datetime(year, month, day)
        # GeÃ§miÅŸ tarihse bir sonraki yÄ±la al
        if dt < datetime.now():
            dt = datetime(year + 1, month, day)
        return dt.strftime('%Y-%m-%d')
    except Exception:
        return None

def normalize_event_date(event_date_str):
    """
    Etkinlik tarihini ISO formatÄ±na (YYYY-MM-DD) Ã§evir.
    GiriÅŸ Ã¶rnekleri: "23 Ekim 2025", "23 Ekim", "23 Ekim Per - 20:00"
    Ã‡Ä±kÄ±ÅŸ: "2025-10-23" veya None (baÅŸarÄ±sÄ±zsa)
    """
    if not event_date_str:
        return None
    
    text = str(event_date_str).lower().strip()
    
    month_to_num = {
        'ocak': 1, 'ÅŸubat': 2, 'subat': 2, 'mart': 3, 'nisan': 4, 
        'mayÄ±s': 5, 'mayis': 5, 'haziran': 6, 'temmuz': 7, 
        'aÄŸustos': 8, 'agustos': 8, 'eylÃ¼l': 9, 'eylul': 9,
        'ekim': 10, 'kasÄ±m': 11, 'kasim': 11, 'aralÄ±k': 12, 'aralik': 12,
    }
    
    # GÃ¼n + Ay + (opsiyonel YÄ±l) formatÄ±nÄ± ara
    pattern = r"(\d{1,2})\s+(ocak|ÅŸubat|subat|mart|nisan|mayÄ±s|mayis|haziran|temmuz|aÄŸustos|agustos|eylÃ¼l|eylul|ekim|kasÄ±m|kasim|aralÄ±k|aralik)(?:\s+(\d{4}))?"
    match = re.search(pattern, text)
    
    if match:
        try:
            day = int(match.group(1))
            month = month_to_num.get(match.group(2))
            year = int(match.group(3)) if match.group(3) else datetime.now().year
            
            if month and 1 <= day <= 31:
                dt = datetime(year, month, day)
                # GeÃ§miÅŸ tarihse bir sonraki yÄ±la al
                if dt < datetime.now():
                    dt = datetime(year + 1, month, day)
                return dt.strftime('%Y-%m-%d')
        except ValueError:
            pass
    
    return None

def categorize_with_gemini(event_title, description=""):
    """Gemini AI ile etkinlik kategorisini belirler"""
    if not model:
        return categorize_simple(event_title)
    
    try:
        prompt = f"""
        AÅŸaÄŸÄ±daki etkinlik baÅŸlÄ±ÄŸÄ±nÄ± ve aÃ§Ä±klamasÄ±nÄ± analiz et ve sadece TEK bir kategori dÃ¶ndÃ¼r:
        
        BaÅŸlÄ±k: {event_title}
        AÃ§Ä±klama: {description[:200]}
        
        Kategori seÃ§enekleri:
        - music (konser, mÃ¼zik, band, DJ, festival)
        - theater (tiyatro, oyun, drama)
        - exhibition (sergi, galeri, mÃ¼ze)
        - workshop (workshop, atÃ¶lye, eÄŸitim, seminer)
        - sports (spor, maÃ§, futbol, basketbol)
        - cinema (sinema, film)
        
        Sadece kategori adÄ±nÄ± dÃ¶ndÃ¼r, baÅŸka bir ÅŸey yazma.
        """
        
        response = model.generate_content(prompt)
        category = response.text.strip().lower()
        
        valid_categories = ['music', 'theater', 'exhibition', 'workshop', 'sports', 'cinema']
        if category in valid_categories:
            return category
        else:
            return categorize_simple(event_title)
            
    except Exception as e:
        logger.error(f"Gemini API hatasÄ±: {e}")
        return categorize_simple(event_title)

def categorize_simple(event_title):
    """Basit keyword bazlÄ± kategorizasyon (fallback)"""
    title_lower = str(event_title).lower()
    
    music_keywords = ['konser', 'mÃ¼zik', 'concert', 'festival', 'band', 'dj', 'canlÄ± mÃ¼zik']
    theater_keywords = ['tiyatro', 'oyun', 'theater', 'sahne', 'drama']
    exhibition_keywords = ['sergi', 'exhibition', 'galeri', 'mÃ¼ze', 'sanat']
    workshop_keywords = ['workshop', 'atÃ¶lye', 'eÄŸitim', 'seminer', 'kurs']
    sports_keywords = ['maÃ§', 'spor', 'futbol', 'basketbol', 'voleybol', 'sports']
    cinema_keywords = ['sinema', 'film', 'cinema', 'movie']
    
    if any(keyword in title_lower for keyword in music_keywords):
        return 'music'
    elif any(keyword in title_lower for keyword in theater_keywords):
        return 'theater'
    elif any(keyword in title_lower for keyword in exhibition_keywords):
        return 'exhibition'
    elif any(keyword in title_lower for keyword in workshop_keywords):
        return 'workshop'
    elif any(keyword in title_lower for keyword in sports_keywords):
        return 'sports'
    elif any(keyword in title_lower for keyword in cinema_keywords):
        return 'cinema'
    else:
        return 'music'  # Default

# ============ SCRAPERS ============

def scrape_biletinial():
    """Biletinial sitesinden Antalya etkinliklerini Ã§eker"""
    logger.info("ğŸ”„ Biletinial scraping baÅŸladÄ± (Antalya)...")
    events = []
    
    base_url = "https://biletinial.com"
    target_urls = {
        'antalya_sinema': 'https://biletinial.com/tr-tr/sinema/antalya',
        'antalya_events': 'https://biletinial.com/tr-tr/sehrineozel/antalya'
    }
    
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
    }
    
    session = requests.Session()
    session.headers.update(headers)
    
    try:
        for url_key, url in target_urls.items():
            logger.info(f"ğŸ“¡ {url_key} Ã§ekiliyor: {url}")
            
            try:
                response = session.get(url, timeout=20)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # FarklÄ± selektÃ¶rleri dene
                selectors = [
                    "a[href*='/tiyatro/']",
                    "a[href*='/sinema/']",
                    "a[href*='/etkinlik/']",
                ]
                
                found_links = []
                for selector in selectors:
                    links = soup.select(selector)
                    if links:
                        found_links.extend(links)
                
                logger.info(f"   â†’ {len(found_links)} etkinlik linki bulundu")
                
                seen_urls = set()
                
                for link in found_links[:30]:  # Ä°lk 30 link
                    try:
                        event_data = {
                            'title': '',
                            'date': '',
                            'time': '',
                            'venue': '',
                            'address': '',
                            'city': 'antalya',
                            'price': '',
                            'description': '',
                            'url': '',
                            'source': 'Biletinial'
                        }
                        
                        # URL
                        href = link.get('href')
                        if href:
                            full_url = href if href.startswith('http') else urljoin(base_url, href)
                            event_data['url'] = full_url
                            if event_data['url'] in seen_urls:
                                continue
                            seen_urls.add(event_data['url'])
                        
                        # BaÅŸlÄ±k
                        event_data['title'] = clean_text(link.get_text(" ")) or clean_text(link.get('title', ''))
                        
                        if not event_data['title']:
                            title_selectors = ['h3', 'h4', 'h5', '.title', '[class*="title"]']
                            for sel in title_selectors:
                                title_el = link.select_one(sel)
                                if title_el:
                                    event_data['title'] = clean_text(title_el.get_text(" "))
                                    break
                        
                        # Gereksiz linkleri filtrele
                        if event_data['title'] and any(skip in event_data['title'].lower() for skip in ['tÃ¼mÃ¼nÃ¼', 'mÃ¼ÅŸteri hizmet', 'keÅŸfet']):
                            continue
                        
                        if event_data['title'] and len(event_data['title']) > 3 and event_data['url']:
                            events.append(event_data)
                            logger.info(f"   âœ… Etkinlik {len(events)}: {event_data['title'][:50]}...")
                    
                    except Exception as e:
                        logger.debug(f"Link iÅŸleme hatasÄ±: {e}")
                        continue
            
            except Exception as e:
                logger.error(f"   âŒ {url_key} Ã§ekilemedi: {e}")
                continue
        
        logger.info(f"âœ… Biletinial'ten {len(events)} etkinlik Ã§ekildi")
        
    except Exception as e:
        logger.error(f"âŒ Biletinial scraping hatasÄ±: {e}")
    
    return events

def scrape_bubilet():
    """BUBilet sitesinden Antalya etkinliklerini Ã§eker"""
    logger.info("ğŸ”„ BUBilet scraping baÅŸladÄ± (Antalya)...")
    events = []
    
    base_url = "https://www.bubilet.com.tr"
    target_url = 'https://www.bubilet.com.tr/antalya'
    
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
    }
    
    session = requests.Session()
    session.headers.update(headers)
    
    try:
        logger.info(f"ğŸ“¡ Antalya Ã§ekiliyor: {target_url}")
        
        response = session.get(target_url, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Etkinlik linklerini bul
        event_links = soup.select("a[href*='/etkinlik/']")
        logger.info(f"   â†’ {len(event_links)} etkinlik linki bulundu")
        
        seen_urls = set()
        
        for link in event_links:
            try:
                event_data = {
                    'title': '',
                    'date': '',
                    'time': '',
                    'venue': '',
                    'address': '',
                    'city': 'antalya',
                    'price': '',
                    'description': '',
                    'url': '',
                    'source': 'BUBilet'
                }
                
                # URL
                href = link.get('href')
                if href:
                    full_url = href if href.startswith('http') else urljoin(base_url, href)
                    event_data['url'] = full_url
                    if event_data['url'] in seen_urls:
                        continue
                    seen_urls.add(event_data['url'])
                
                # BaÅŸlÄ±k (h3)
                h3 = link.select_one("h3")
                if h3:
                    event_data['title'] = clean_text(h3.get_text(" "))
                
                # Title from attribute fallback
                if not event_data['title']:
                    title_attr = link.get('title')
                    if title_attr:
                        event_data['title'] = clean_text(title_attr)
                
                # Yer ve Tarih (p tags)
                p_tags = link.select("p.text-gray-500")
                if len(p_tags) >= 1:
                    event_data['venue'] = clean_text(p_tags[0].get_text(" "))
                if len(p_tags) >= 2:
                    date_text = clean_text(p_tags[1].get_text(" "))
                    # Parse tarih ISO formatÄ±na
                    parsed = parse_date_from_text(date_text)
                    if parsed:
                        event_data['date'] = parsed
                    else:
                        event_data['date'] = date_text
                
                # Fiyat (span.tracking-tight)
                price_span = link.select_one("span.tracking-tight")
                if price_span:
                    price_text = clean_text(price_span.get_text(" "))
                    if price_text and not price_text.endswith('â‚º'):
                        event_data['price'] = price_text + " â‚º"
                    else:
                        event_data['price'] = price_text
                
                if event_data['title'] and len(event_data['title']) > 3 and event_data['url']:
                    events.append(event_data)
                    logger.info(f"   âœ… Etkinlik {len(events)}: {event_data['title'][:50]}...")
            
            except Exception as e:
                logger.debug(f"Link iÅŸleme hatasÄ±: {e}")
                continue
        
        logger.info(f"âœ… BUBilet'ten {len(events)} etkinlik Ã§ekildi")
        
    except Exception as e:
        logger.error(f"âŒ BUBilet scraping hatasÄ±: {e}")
    
    return events

def scrape_biletix():
    """Biletix sitesinden Antalya etkinliklerini Ã§eker (Selenium ile dinamik iÃ§erik)"""
    logger.info("ğŸ”„ Biletix scraping baÅŸladÄ± (Antalya)...")
    events = []
    
    base_url = "https://www.biletix.com"
    # Antalya iÃ§in Ã¶zel arama sayfasÄ± (Ã¶rnek scraper'dan alÄ±ndÄ±)
    target_url = 'https://www.biletix.com/search/TURKIYE/tr?category_sb=-1&date_sb=-1&city_sb=Antalya#!city_sb:Antalya'
    
    # Selenium ile dinamik iÃ§erik yÃ¼kleme
    try:
        from selenium import webdriver
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.chrome.options import Options
        from selenium.common.exceptions import TimeoutException, NoSuchElementException
        from webdriver_manager.chrome import ChromeDriverManager
        from selenium.webdriver.chrome.service import Service
    except ImportError:
        logger.warning("âš ï¸  Selenium bulunamadÄ±, Biletix scraping atlanÄ±yor. 'pip install selenium webdriver-manager' ile yÃ¼kleyin.")
        return events
    
    driver = None
    try:
        logger.info(f"ğŸ“¡ Biletix Antalya Ã§ekiliyor (Selenium ile): {target_url}")
        
        # Headless Chrome ayarlarÄ±
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # ChromeDriver'Ä± otomatik yÃ¼kle
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
        except Exception as e:
            logger.warning(f"âš ï¸  ChromeDriver yÃ¼klenemedi: {e}. Sistem ChromeDriver kullanÄ±lacak.")
            driver = webdriver.Chrome(options=chrome_options)
        
        # SayfayÄ± yÃ¼kle
        logger.info(f"   â†’ URL aÃ§Ä±lÄ±yor: {target_url}")
        driver.get(target_url)
        
        # Sayfa yÃ¼klenmesini bekle
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            logger.warning("   âš ï¸  Sayfa yÃ¼klenme timeout, devam ediliyor...")
        
        logger.info("   â†’ Sayfa yÃ¼klendi, dinamik iÃ§erik bekleniyor...")
        time.sleep(15)  # Ã–rnek scraper'da 15 saniye bekleniyor
        
        # Sayfa baÅŸlÄ±ÄŸÄ±nÄ± kontrol et
        page_title = driver.title
        logger.info(f"   â†’ Sayfa baÅŸlÄ±ÄŸÄ±: {page_title}")
        
        # URL'nin deÄŸiÅŸip deÄŸiÅŸmediÄŸini kontrol et (redirect olabilir)
        current_url = driver.current_url
        if current_url != target_url:
            logger.info(f"   â†’ URL redirect oldu: {current_url}")
            # EÄŸer 404'e yÃ¶nlendirildiyse, farklÄ± bir URL dene
            if '404' in current_url.lower():
                logger.warning("   âš ï¸  404 hatasÄ± alÄ±ndÄ±, alternatif URL deneniyor...")
                # Alternatif URL'yi dene
                alt_url = 'https://www.biletix.com/anasayfa/ETKINLIK/tr'
                logger.info(f"   â†’ Alternatif URL deneniyor: {alt_url}")
                driver.get(alt_url)
                time.sleep(15)
        
        # SayfayÄ± kademeli scroll et (lazy loading iÃ§in) - Ã¶rnek scraper'dan
        logger.info("   â†’ Sayfa scroll ediliyor...")
        for i in range(5):  # Ã–rnek scraper'da 5 kez scroll ediliyor
            driver.execute_script(f"window.scrollTo(0, {(i+1) * 800});")
            time.sleep(2)
        
        # Son scroll
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(5)  # Ã–rnek scraper'da 5 saniye bekleniyor
        
        # Etkinlik kartlarÄ±nÄ± bul - Ã¶rnek scraper'dan alÄ±ndÄ±
        # Ã–nce ana selector'Ä± dene: .flexibleEvent
        event_cards = driver.find_elements(By.CSS_SELECTOR, ".flexibleEvent")
        logger.info(f"   â†’ .flexibleEvent ile {len(event_cards)} etkinlik kartÄ± bulundu")
        
        # EÄŸer bulunamadÄ±ysa alternatif selector'larÄ± dene
        if not event_cards:
            logger.warning("   âš ï¸  .flexibleEvent ile kart bulunamadÄ±, alternatif selector'lar deneniyor...")
            event_cards = driver.find_elements(By.CSS_SELECTOR, 
                ".searchResultEvent, .listevent, .event, .card, .item")
            logger.info(f"   â†’ Alternatif selector'lar ile {len(event_cards)} etkinlik kartÄ± bulundu")
        
        # KartlarÄ±n iÃ§eriÄŸini kontrol et (Ã¶rnek scraper'dan)
        valid_cards = []
        for i, card in enumerate(event_cards):
            try:
                # KartÄ±n iÃ§eriÄŸini kontrol et
                card_html = card.get_attribute('innerHTML')
                if card_html and len(card_html.strip()) > 50:  # En az 50 karakter iÃ§erik
                    valid_cards.append(card)
                    logger.debug(f"   â†’ Kart {i+1}: Ä°Ã§erik bulundu ({len(card_html)} karakter)")
                else:
                    logger.debug(f"   â†’ Kart {i+1}: BoÅŸ iÃ§erik")
            except Exception as e:
                logger.debug(f"   â†’ Kart {i+1}: Kontrol hatasÄ± - {e}")
        
        logger.info(f"   â†’ Ä°Ã§erikli kart sayÄ±sÄ±: {len(valid_cards)}")
        event_cards = valid_cards  # Sadece iÃ§erikli kartlarÄ± kullan
        
        # EÄŸer hala bulunamadÄ±ysa, link bazlÄ± arama yap
        if not event_cards:
            logger.warning("   âš ï¸  Etkinlik kartÄ± bulunamadÄ±, sayfa yapÄ±sÄ± analiz ediliyor...")
            
            # Sayfa kaynaÄŸÄ±nÄ± kontrol et
            page_source = driver.page_source
            logger.debug(f"   â†’ Sayfa kaynaÄŸÄ± uzunluÄŸu: {len(page_source)} karakter")
            
            # TÃ¼m linkleri kontrol et
            all_links = driver.find_elements(By.TAG_NAME, "a")
            logger.info(f"   â†’ Sayfada toplam {len(all_links)} link bulundu")
            
            event_cards = []
            for link in all_links:
                try:
                    href = link.get_attribute('href')
                    if href:
                        # Biletix URL pattern'lerini kontrol et
                        if any(pattern in href.lower() for pattern in ['/etkinlik/', '/event/', 'biletix.com/anasayfa/', 'biletix.com/search']):
                            # Ana sayfa ve arama sayfalarÄ±nÄ± filtrele
                            if '/anasayfa/' not in href and '/search' not in href:
                                event_cards.append(link)
                except:
                    continue
            
            logger.info(f"   â†’ Link bazlÄ± arama ile {len(event_cards)} potansiyel etkinlik bulundu")
            
            # EÄŸer hala bulunamadÄ±ysa, sayfanÄ±n body iÃ§eriÄŸini kontrol et
            if not event_cards:
                try:
                    body_text = driver.find_element(By.TAG_NAME, "body").text
                    logger.debug(f"   â†’ Body metni uzunluÄŸu: {len(body_text)} karakter")
                    if len(body_text) < 100:
                        logger.warning("   âš ï¸  Sayfa iÃ§eriÄŸi Ã§ok kÄ±sa, muhtemelen yÃ¼klenmedi veya hata var")
                except:
                    pass
        
        seen_urls = set()
        
        for i, card in enumerate(event_cards[:20]):  # Ä°lk 20 geÃ§erli etkinliÄŸi al (Ã¶rnek scraper'dan)
            try:
                logger.info(f"   â†’ Etkinlik {i+1} Ã§Ä±karÄ±lÄ±yor...")
                
                # Ã–nce kartÄ±n tÃ¼m metnini al (Ã¶rnek scraper'dan)
                all_text = clean_text(card.text)
                logger.debug(f"      â†’ Kart metni: {all_text[:100]}...")
                
                event_data = {
                    'title': '',
                    'date': '',
                    'time': '',
                    'venue': '',
                    'address': '',
                    'city': 'antalya',
                    'price': '',
                    'description': '',
                    'url': '',
                    'source': 'Biletix'
                }
                
                # URL bul (Ã¶rnek scraper'dan)
                try:
                    link_elem = card.find_element(By.TAG_NAME, 'a')
                    href = link_elem.get_attribute('href')
                    if href:
                        event_data['url'] = href if href.startswith('http') else urljoin(base_url, href)
                        if event_data['url'] in seen_urls:
                            logger.debug(f"      â†’ Duplicate URL, atlanÄ±yor: {event_data['url']}")
                            continue
                        seen_urls.add(event_data['url'])
                        logger.debug(f"      â†’ URL bulundu: {event_data['url']}")
                except NoSuchElementException:
                    logger.debug(f"      â†’ Link bulunamadÄ±")
                    pass
                
                # BaÅŸlÄ±k bul - Ã¶rnek scraper'dan (daha geniÅŸ selector'lar)
                title_selectors = [
                    '.event-title', '.title', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                    '[data-testid*="title"]', '.event-name', '.name', '.heading',
                    'a[href*="/etkinlik/"]', 'a[href*="/event/"]'
                ]
                
                for selector in title_selectors:
                    try:
                        title_elem = card.find_element(By.CSS_SELECTOR, selector)
                        title_text = clean_text(title_elem.text)
                        if title_text and len(title_text) > 3:
                            event_data['title'] = title_text
                            logger.debug(f"      â†’ BaÅŸlÄ±k bulundu: {title_text[:50]}...")
                            break
                    except NoSuchElementException:
                        continue
                
                # EÄŸer baÅŸlÄ±k bulunamazsa, link metnini kullan (Ã¶rnek scraper'dan)
                if not event_data['title']:
                    try:
                        link_elem = card.find_element(By.TAG_NAME, 'a')
                        link_text = clean_text(link_elem.text)
                        if link_text and len(link_text) > 3:
                            event_data['title'] = link_text
                            logger.debug(f"      â†’ Link metni baÅŸlÄ±k olarak kullanÄ±ldÄ±: {link_text[:50]}...")
                    except NoSuchElementException:
                        pass
                
                # Tarih bul - Ã¶rnek scraper'dan
                date_selectors = [
                    '.event-date', '.date', '.event-time', '.time', '.datetime',
                    '[data-testid*="date"]', '.tarih', '.tarih-saat'
                ]
                
                for selector in date_selectors:
                    try:
                        date_elem = card.find_element(By.CSS_SELECTOR, selector)
                        date_text = clean_text(date_elem.text)
                        if date_text:
                            # Parse tarih ISO formatÄ±na
                            parsed = parse_date_from_text(date_text)
                            if parsed:
                                event_data['date'] = parsed
                            else:
                                event_data['date'] = date_text
                            logger.debug(f"      â†’ Tarih bulundu: {date_text}")
                            break
                    except NoSuchElementException:
                        continue
                
                # Konum/Venue bul - Ã¶rnek scraper'dan
                location_selectors = [
                    '.event-location', '.location', '.venue', '.place', '.mekan',
                    '[data-testid*="location"]', '.adres', '.yer'
                ]
                
                for selector in location_selectors:
                    try:
                        location_elem = card.find_element(By.CSS_SELECTOR, selector)
                        location_text = clean_text(location_elem.text)
                        if location_text:
                            event_data['venue'] = location_text
                            logger.debug(f"      â†’ Konum bulundu: {location_text}")
                            break
                    except NoSuchElementException:
                        continue
                
                # Fiyat bul - Ã¶rnek scraper'dan
                price_selectors = [
                    '.event-price', '.price', '.ticket-price', '.fiyat', '.cost',
                    '[data-testid*="price"]', '.amount', '.fee'
                ]
                
                for selector in price_selectors:
                    try:
                        price_elem = card.find_element(By.CSS_SELECTOR, selector)
                        price_text = clean_text(price_elem.text)
                        if price_text:
                            event_data['price'] = price_text
                            logger.debug(f"      â†’ Fiyat bulundu: {price_text}")
                            break
                    except NoSuchElementException:
                        continue
                
                # EÄŸer hiÃ§bir ÅŸey bulunamazsa, kartÄ±n tÃ¼m metnini analiz et (Ã¶rnek scraper'dan)
                if not event_data['title'] and all_text:
                    # Basit metin analizi ile baÅŸlÄ±k Ã§Ä±kar
                    lines = [line.strip() for line in all_text.split('\n') if line.strip()]
                    if lines:
                        # En uzun satÄ±rÄ± baÅŸlÄ±k olarak kullan
                        event_data['title'] = max(lines, key=len)
                        logger.debug(f"      â†’ Metin analizi ile baÅŸlÄ±k: {event_data['title'][:50]}...")
                
                # Gereksiz linkleri filtrele
                if event_data['title'] and any(skip in event_data['title'].lower() for skip in ['tÃ¼mÃ¼nÃ¼', 'mÃ¼ÅŸteri hizmet', 'keÅŸfet', 'daha fazla']):
                    logger.debug(f"      â†’ Gereksiz link filtrelendi: {event_data['title']}")
                    continue
                
                # En az baÅŸlÄ±k varsa etkinliÄŸi kabul et (Ã¶rnek scraper'dan)
                if event_data['title'] and len(event_data['title']) > 3:
                    events.append(event_data)
                    logger.info(f"   âœ… Etkinlik {len(events)}: {event_data['title'][:50]}...")
                else:
                    logger.debug(f"      â†’ Etkinlik verisi Ã§Ä±karÄ±lamadÄ± (baÅŸlÄ±k yok veya Ã§ok kÄ±sa)")
            
            except Exception as e:
                logger.error(f"   âŒ Etkinlik {i+1} Ã§Ä±karÄ±lÄ±rken hata: {e}")
                continue
        
        logger.info(f"âœ… Biletix'ten {len(events)} etkinlik Ã§ekildi")
        
    except Exception as e:
        logger.error(f"âŒ Biletix scraping hatasÄ±: {e}")
        import traceback
        logger.debug(traceback.format_exc())
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return events

# ============ PROCESS AND SAVE ============

def process_and_save_events(raw_events):
    """Etkinlikleri iÅŸler ve veritabanÄ±na kaydeder"""
    logger.info(f"ğŸ“Š Toplam {len(raw_events)} etkinlik iÅŸleniyor...")
    
    saved_count = 0
    updated_count = 0
    
    for event in raw_events:
        try:
            # Tarih normalleÅŸtir
            date_str = event.get('date', '')
            if date_str:
                normalized_date = normalize_event_date(date_str)
                if normalized_date:
                    event['date'] = normalized_date
                elif not date_str.startswith('202'):  # EÄŸer zaten ISO formatÄ±nda deÄŸilse
                    # Tarih parse edilemediyse, bugÃ¼nden 7 gÃ¼n sonrasÄ±nÄ± varsay
                    event['date'] = (datetime.now() + timedelta(days=7)).strftime('%Y-%m-%d')
            else:
                # Tarih yoksa, bugÃ¼nden 7 gÃ¼n sonrasÄ±nÄ± varsay
                event['date'] = (datetime.now() + timedelta(days=7)).strftime('%Y-%m-%d')
            
            # Kategori belirle
            category = categorize_with_gemini(event.get('title', ''), event.get('description', ''))
            
            # Etkinlik objesi oluÅŸtur
            event_data = {
                'title': event.get('title', '').strip(),
                'description': event.get('description', ''),
                'city': 'antalya',  # Sadece Antalya
                'category': category,
                'date': event['date'],
                'time': event.get('time', ''),
                'venue': event.get('venue', ''),
                'address': event.get('address', ''),
                'price': event.get('price', 'Bilet iÃ§in tÄ±klayÄ±n'),
                'url': event.get('url', ''),
                'image_url': event.get('image_url', ''),
                'organizer': event.get('source', 'Bilinmiyor'),
                'tags': [],
                'source': event.get('source', 'scraper'),
                'last_scraped': datetime.now(),
                'updated_at': datetime.now()
            }
            
            # BaÅŸlÄ±k yoksa atla
            if not event_data['title'] or len(event_data['title']) < 3:
                continue
            
            # AynÄ± baÅŸlÄ±k ve tarih varsa gÃ¼ncelle, yoksa ekle
            existing = events_collection.find_one({
                'title': event_data['title'],
                'date': event_data['date']
            })
            
            if existing:
                events_collection.update_one(
                    {'_id': existing['_id']},
                    {'$set': event_data}
                )
                updated_count += 1
            else:
                event_data['created_at'] = datetime.now()
                events_collection.insert_one(event_data)
                saved_count += 1
        
        except Exception as e:
            logger.error(f"Etkinlik kaydetme hatasÄ±: {e}")
            continue
    
    logger.info(f"âœ… {saved_count} yeni etkinlik eklendi, {updated_count} etkinlik gÃ¼ncellendi")
    return saved_count, updated_count

def clean_old_events():
    """GeÃ§miÅŸ tarihlerdeki etkinlikleri temizler"""
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    result = events_collection.delete_many({'date': {'$lt': yesterday}})
    
    logger.info(f"ğŸ—‘ï¸ {result.deleted_count} eski etkinlik silindi")
    return result.deleted_count

# ============ MAIN SCRAPER FUNCTION ============

def run_scraper():
    """Ana scraper fonksiyonu - Sadece Antalya iÃ§in"""
    logger.info("=" * 50)
    logger.info("ğŸš€ Antalya Event Scraper BaÅŸlatÄ±ldÄ±")
    logger.info("=" * 50)
    
    start_time = time.time()
    all_events = []
    
    # Biletinial'den Ã§ek
    try:
        biletinial_events = scrape_biletinial()
        all_events.extend(biletinial_events)
    except Exception as e:
        logger.error(f"Biletinial scraping hatasÄ±: {e}")
    
    # BUBilet'ten Ã§ek
    try:
        bubilet_events = scrape_bubilet()
        all_events.extend(bubilet_events)
    except Exception as e:
        logger.error(f"BUBilet scraping hatasÄ±: {e}")
    
    # Biletix'ten Ã§ek
    try:
        biletix_events = scrape_biletix()
        all_events.extend(biletix_events)
    except Exception as e:
        logger.error(f"Biletix scraping hatasÄ±: {e}")
    
    # VeritabanÄ±na kaydet
    if all_events:
        saved, updated = process_and_save_events(all_events)
    else:
        logger.warning("âš ï¸ HiÃ§ etkinlik bulunamadÄ±!")
        saved, updated = 0, 0
    
    # Eski etkinlikleri temizle
    deleted = clean_old_events()
    
    # Ã–zet
    elapsed_time = time.time() - start_time
    logger.info("=" * 50)
    logger.info(f"âœ… Scraper tamamlandÄ± - SÃ¼re: {elapsed_time:.2f} saniye")
    logger.info(f"ğŸ“Š Toplam: {len(all_events)} etkinlik toplandÄ±")
    logger.info(f"â• Yeni: {saved} | ğŸ”„ GÃ¼ncellenen: {updated} | ğŸ—‘ï¸ Silinen: {deleted}")
    logger.info(f"ğŸ“… VeritabanÄ±nda toplam: {events_collection.count_documents({})} etkinlik")
    logger.info("=" * 50)
    
    return {
        'success': True,
        'total_scraped': len(all_events),
        'saved': saved,
        'updated': updated,
        'deleted': deleted,
        'duration': elapsed_time
    }

if __name__ == '__main__':
    try:
        result = run_scraper()
        exit(0 if result['success'] else 1)
    except Exception as e:
        logger.error(f"âŒ Kritik hata: {e}")
        import traceback
        logger.error(traceback.format_exc())
        exit(1)

"""
FAISS + Embedding Retriever - KullanÄ±cÄ± sorgusuna gÃ¶re en alakalÄ± etkinlikleri bulur
Based on the original rotiva project

RAM Optimizasyonu:
- Model singleton pattern ile paylaÅŸÄ±lÄ±yor (her process'te bir kez yÃ¼kleniyor)
- Model cache mekanizmasÄ± (disk cache)
- Lazy loading (sadece gerektiÄŸinde yÃ¼kleniyor)
"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import logging
import os
import threading

logger = logging.getLogger(__name__)

# Global model cache (process-level singleton)
_model_cache = {}
_model_lock = threading.Lock()


def _get_or_load_model(model_name, cache_dir=None):
    """
    Model'i singleton pattern ile yÃ¼kle (process-level cache)
    Her process'te sadece bir kez yÃ¼klenir, thread-safe
    """
    global _model_cache, _model_lock
    
    with _model_lock:
        if model_name not in _model_cache:
            logger.info(f"ğŸ”„ Embedding model yÃ¼kleniyor: {model_name} (ilk yÃ¼kleme, cache'e alÄ±nÄ±yor)")
            
            # Cache dizini ayarla
            if cache_dir is None:
                cache_dir = os.getenv('HF_HOME', os.path.join(os.path.expanduser('~'), '.cache', 'huggingface'))
            
            # Model'i cache dizininden yÃ¼kle (disk cache kullan)
            try:
                # Model cache mekanizmasÄ± - disk'ten yÃ¼kle
                # RAM optimizasyonu: model'i daha verimli yÃ¼kle
                import torch
                
                # PyTorch bellek optimizasyonu
                torch.set_num_threads(1)  # Tek thread kullan (bellek tasarrufu)
                
                model = SentenceTransformer(
                    model_name,
                    cache_folder=cache_dir,
                    device='cpu',  # CPU kullan (GPU yoksa)
                    model_kwargs={
                        'low_cpu_mem_usage': True,  # DÃ¼ÅŸÃ¼k bellek kullanÄ±mÄ±
                    }
                )
                
                # Model'i eval moduna al (training modundan daha az bellek kullanÄ±r)
                model.eval()
                
                # PyTorch cache'i temizle (bellek tasarrufu)
                if hasattr(torch, 'empty_cache'):
                    torch.empty_cache()
                
                logger.info(f"âœ… Embedding model hazÄ±r (boyut: {model.get_sentence_embedding_dimension()}, cache: {cache_dir}, RAM optimized)")
                _model_cache[model_name] = model
            except Exception as e:
                logger.error(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
                raise
        
        return _model_cache[model_name]


class FAISSRetriever:
    """
    FAISS vektor database + Sentence-Transformers embedding modeli ile semantik arama
    
    âœ… Embedding Model: paraphrase-multilingual-MiniLM-L12-v2 (TÃ¼rkÃ§e desteÄŸi)
    âœ… Vektor Database: FAISS (Facebook AI Similarity Search)
    âœ… RAM Optimizasyonu: Model singleton pattern ile paylaÅŸÄ±lÄ±yor
    """
    
    def __init__(self, events, model_name=None):
        """
        Args:
            events: Etkinlik listesi (MongoDB cursor veya list)
            model_name: Huggingface embedding model (varsayÄ±lan: daha kÃ¼Ã§Ã¼k model - RAM tasarrufu iÃ§in)
        """
        # Daha kÃ¼Ã§Ã¼k model kullan (RAM tasarrufu iÃ§in)
        # paraphrase-multilingual-MiniLM-L12-v2: ~120MB, 384 dim
        # all-MiniLM-L6-v2: ~80MB, 384 dim (Ä°ngilizce odaklÄ± ama Ã§ok daha kÃ¼Ã§Ã¼k)
        # TÃ¼rkÃ§e iÃ§in: paraphrase-multilingual-MiniLM-L12-v2 (daha bÃ¼yÃ¼k ama TÃ¼rkÃ§e desteÄŸi var)
        if model_name is None:
            # RAM tasarrufu iÃ§in daha kÃ¼Ã§Ã¼k model kullan
            # EÄŸer TÃ¼rkÃ§e desteÄŸi kritikse, paraphrase-multilingual-MiniLM-L12-v2 kullan
            model_name = os.getenv('EMBEDDING_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')
            # TÃ¼rkÃ§e desteÄŸi iÃ§in: 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        # Convert MongoDB cursor to list if needed
        if hasattr(events, '__iter__') and not isinstance(events, (list, tuple)):
            self.events = list(events)
        else:
            self.events = events
        
        # Model'i singleton pattern ile yÃ¼kle (process-level cache)
        cache_dir = os.getenv('HF_HOME', '/app/model_cache')
        self.model = _get_or_load_model(model_name, cache_dir=cache_dir)
        
        # Her etkinlik iÃ§in aranabilir metin oluÅŸtur
        self.texts = []
        for e in self.events:
            # Create searchable text from event fields
            searchable_text = f"{e.get('title', '')} {e.get('description', '')} {e.get('city', '')} {e.get('category', '')} {e.get('venue', '')}"
            self.texts.append(searchable_text)
        
        # ğŸ“Š TÃ¼m etkinlikleri embedding'e Ã§evir (vektÃ¶r temsili)
        # Batch processing ile bellek kullanÄ±mÄ±nÄ± optimize et
        logger.info(f"ğŸ”„ {len(self.events)} etkinlik vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
        
        # Batch size ile bellek kullanÄ±mÄ±nÄ± kontrol et (bÃ¼yÃ¼k listeler iÃ§in)
        # Daha kÃ¼Ã§Ã¼k batch size = daha az bellek kullanÄ±mÄ±
        batch_size = 16  # 32'den 16'ya dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ (bellek tasarrufu)
        embeddings_list = []
        
        for i in range(0, len(self.texts), batch_size):
            batch_texts = self.texts[i:i+batch_size]
            # Normalize embeddings ve bellek optimizasyonu
            batch_embeddings = self.model.encode(
                batch_texts, 
                show_progress_bar=False, 
                convert_to_numpy=True,
                normalize_embeddings=True,  # Normalize et (bellek tasarrufu)
                batch_size=8  # Model encode iÃ§in daha kÃ¼Ã§Ã¼k batch
            )
            embeddings_list.append(batch_embeddings.astype('float32'))
            
            # Her batch'ten sonra bellek temizliÄŸi
            if i % (batch_size * 4) == 0:  # Her 4 batch'te bir
                import gc
                gc.collect()
        
        # TÃ¼m batch'leri birleÅŸtir
        self.embeddings = np.vstack(embeddings_list).astype('float32')
        
        # GeÃ§ici listeyi temizle
        embeddings_list = None
        import gc
        gc.collect()
        
        # ğŸ—„ï¸ FAISS vektor database oluÅŸtur (hÄ±zlÄ± benzerlik aramasÄ± iÃ§in)
        dimension = self.embeddings.shape[1]  # VektÃ¶r boyutu (384)
        self.index = faiss.IndexFlatL2(dimension)  # L2 mesafe metriÄŸi
        self.index.add(self.embeddings)  # VektÃ¶rleri FAISS'e ekle
        
        logger.info(f"âœ… FAISS Retriever hazÄ±r: {len(self.events)} etkinlik indekslendi (RAM optimized)")
    
    def retrieve(self, query, k=5, city_filter=None):
        """
        KullanÄ±cÄ± sorgusuna en yakÄ±n etkinlikleri bul (semantik arama)
        
        Args:
            query: KullanÄ±cÄ±nÄ±n arama sorgusu
            k: KaÃ§ etkinlik dÃ¶ndÃ¼rÃ¼lecek (varsayÄ±lan: 5)
            city_filter: Åehir filtresi (Ã¶rn: "antalya")
        
        Returns:
            list: Her biri {'event': dict, 'score': float} iÃ§eren liste (benzerlik skoruna gÃ¶re sÄ±ralÄ±)
        """
        try:
            # ğŸ” KullanÄ±cÄ± sorgusunu embedding'e Ã§evir
            # Normalize embeddings ve bellek optimizasyonu
            query_embedding = self.model.encode(
                [query], 
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )[0].astype('float32')
            
            # ğŸ—„ï¸ FAISS ile en yakÄ±n vektÃ¶rleri bul (k*2 al, filtreleme iÃ§in)
            distances, indices = self.index.search(
                query_embedding.reshape(1, -1),
                min(k * 2, len(self.events))
            )
            
            results = []
            for dist, idx in zip(distances[0], indices[0]):
                event = self.events[idx]
                
                # Åehir filtresi varsa uygula
                if city_filter:
                    event_city = str(event.get('city', '')).lower()
                    if event_city != city_filter.lower():
                        continue
                
                # FAISS L2 mesafesini benzerlik skoruna Ã§evir (0-1 arasÄ±)
                # DÃ¼ÅŸÃ¼k mesafe = yÃ¼ksek benzerlik
                similarity_score = 1 / (1 + float(dist))
                
                results.append({
                    'event': event,
                    'score': similarity_score
                })
                
                # Yeterli sonuÃ§ toplandÄ±ysa dur
                if len(results) >= k:
                    break
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Retrieval hatasÄ±: {e}")
            return []

